import requests
from bs4 import BeautifulSoup
from textblob import TextBlob
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, date
import xml.etree.ElementTree as ET
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_stopwords
import json
import os
from collections import Counter

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ matplotlib ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
plt.rcParams['font.family'] = 'TH Sarabun New'  # ‡∏´‡∏£‡∏∑‡∏≠ 'Tahoma'

# ‡∏Ç‡∏¢‡∏≤‡∏¢ Thai Sentiment Lexicon ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
THAI_SENTIMENT_LEXICON = {
    # ‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å‡∏°‡∏≤‡∏Å (0.8 - 1.0)
    "‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°": 1.0, "‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏¢‡∏≠‡∏î": 1.0, "‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î": 1.0, "‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°": 1.0, 
    "‡πÄ‡∏à‡∏£‡∏¥‡∏ç": 0.9, "‡∏£‡∏∏‡πà‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡∏á": 0.9, "‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï": 0.9, "‡∏û‡∏∏‡πà‡∏á": 0.9, "‡∏ó‡∏∞‡∏¢‡∏≤‡∏ô": 0.9,
    "‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à": 0.8, "‡∏ä‡∏ô‡∏∞": 0.8, "‡πÑ‡∏î‡πâ": 0.8, "‡∏î‡∏µ": 0.8, "‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°": 0.9,
    
    # ‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (0.4 - 0.7)
    "‡∏ä‡∏≠‡∏ö": 0.7, "‡∏û‡∏≠‡πÉ‡∏à": 0.7, "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ": 0.7, "‡∏î‡∏µ‡πÉ‡∏à": 0.7, "‡∏™‡∏î‡πÉ‡∏™": 0.7,
    "‡∏Ç‡∏∂‡πâ‡∏ô": 0.6, "‡πÄ‡∏û‡∏¥‡πà‡∏°": 0.6, "‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô": 0.6, "‡∏ü‡∏∑‡πâ‡∏ô‡∏ï‡∏±‡∏ß": 0.6, "‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á": 0.6,
    "‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á": 0.5, "‡∏£‡∏≤‡∏ö‡∏£‡∏∑‡πà‡∏ô": 0.5, "‡∏õ‡∏Å‡∏ï‡∏¥": 0.4, "‡πÇ‡∏≠‡πÄ‡∏Ñ": 0.4,
    
    # ‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö‡∏°‡∏≤‡∏Å (-0.8 ‡∏ñ‡∏∂‡∏á -1.0)
    "‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å": -1.0, "‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß": -1.0, "‡πÄ‡∏à‡πä‡∏á": -1.0, "‡∏•‡πà‡∏°‡∏™‡∏•‡∏≤‡∏¢": -1.0, "‡∏ß‡∏¥‡∏Å‡∏§‡∏ï": -1.0,
    "‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï": -0.9, "‡πÇ‡∏Å‡∏á": -0.9, "‡∏â‡πâ‡∏≠‡πÇ‡∏Å‡∏á": -0.9, "‡∏Ñ‡∏≠‡∏£‡πå‡∏£‡∏±‡∏õ‡∏ä‡∏±‡πà‡∏ô": -0.9, "‡∏´‡∏•‡∏≠‡∏Å‡∏•‡∏ß‡∏á": -0.9,
    "‡∏Ç‡∏≤‡∏î‡∏ó‡∏∏‡∏ô": -0.9, "‡∏ï‡∏Å‡∏ï‡πà‡∏≥": -0.9, "‡∏¢‡πà‡∏≥‡πÅ‡∏¢‡πà": -0.9, "‡∏ï‡∏Å‡∏Å‡∏£‡∏∞‡∏õ‡πã‡∏≠‡∏á": -0.9, "‡∏î‡∏¥‡πà‡∏á": -0.9,
    "‡πÅ‡∏¢‡πà": -0.8, "‡∏™‡πÅ‡∏Å‡∏°": -0.8, "‡∏™‡πÅ‡∏Å‡∏°‡πÄ‡∏°‡∏≠‡∏£‡πå": -0.8, "‡πÄ‡∏™‡∏µ‡∏¢": -0.8, "‡πÄ‡∏•‡∏ß‡∏£‡πâ‡∏≤‡∏¢": -0.8,
    
    # ‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (-0.4 ‡∏ñ‡∏∂‡∏á -0.7)
    "‡∏õ‡∏±‡∏ç‡∏´‡∏≤": -0.7, "‡∏Å‡∏±‡∏á‡∏ß‡∏•": -0.7, "‡∏´‡πà‡∏ß‡∏á": -0.7, "‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á": -0.7, "‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢": -0.7,
    "‡∏•‡∏î‡∏•‡∏á": -0.6, "‡∏•‡∏î": -0.6, "‡∏´‡∏î": -0.6, "‡∏ï‡∏Å": -0.6, "‡∏•‡∏á": -0.6,
    "‡∏≠‡πà‡∏≠‡∏ô‡πÅ‡∏≠": -0.5, "‡∏ä‡∏∞‡∏•‡∏≠": -0.5, "‡∏ã‡∏ö‡πÄ‡∏ã‡∏≤": -0.5, "‡∏ã‡∏∂‡∏°": -0.5, "‡∏ï‡∏¥‡∏î‡∏Ç‡∏±‡∏î": -0.5,
    "‡πÅ‡∏û‡∏á": -0.4, "‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢": -0.4, "‡∏¢‡∏≤‡∏Å": -0.4,
    
    # ‡∏Ñ‡∏≥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô
    "‡∏Å‡∏≥‡πÑ‡∏£": 0.8, "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ": 0.6, "‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏∏‡∏ô": 0.5, "‡∏•‡∏á‡∏ó‡∏∏‡∏ô": 0.5,
    "‡∏´‡∏ô‡∏µ‡πâ": -0.6, "‡∏Ç‡∏≤‡∏î‡∏î‡∏∏‡∏•": -0.7, "‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏ü‡πâ‡∏≠": -0.6, "‡∏ß‡πà‡∏≤‡∏á‡∏á‡∏≤‡∏ô": -0.7,
    
    # ‡∏Ñ‡∏≥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô
    "‡πÅ‡∏Å‡∏ß‡πà‡∏á": 0.0, "‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå": 0.0, "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô": 0.0, "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå": 0.0,
    "‡∏Ç‡∏≤‡∏¢": -0.3, "‡∏ñ‡∏∑‡∏≠": 0.1, "‡∏ã‡∏∑‡πâ‡∏≠": 0.4, "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ã‡∏∑‡πâ‡∏≠": 0.7,
    
    # ‡∏Ñ‡∏≥‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (Intensifiers)
    "‡∏°‡∏≤‡∏Å": 1.2, "‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢": 1.2, "‡∏™‡∏∏‡∏î": 1.3, "‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î": 1.3, "‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ": 1.2,
    "‡πÑ‡∏°‡πà": -1.5, "‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà": -1.5, "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ": -1.5,
}

THAI_STOPWORDS = set(thai_stopwords())

def get_google_news(keyword, lang="th", max_results=100):
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å Google News
    """
    if lang == "th":
        url = f"https://news.google.com/rss/search?q={keyword}&hl=th&gl=TH&ceid=TH:th"
    elif lang == "en":
        url = f"https://news.google.com/rss/search?q={keyword}&hl=en-US&gl=US&ceid=US:en"
    else:
        print("‚ö†Ô∏è ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: 'th' ‡∏´‡∏£‡∏∑‡∏≠ 'en'")
        return []

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error fetching data: {e}")
        return []
    
    try:
        root = ET.fromstring(response.content)
    except ET.ParseError as e:
        print(f"‚ùå Error parsing XML: {e}")
        return []
    
    news_list = []
    
    for item in root.findall('./channel/item'):
        if len(news_list) >= max_results:
            break
            
        title = item.find('title').text if item.find('title') is not None else 'N/A'
        link = item.find('link').text if item.find('link') is not None else 'N/A'
        pub_date = item.find('pubDate').text if item.find('pubDate') is not None else 'N/A'
        source = item.find('source').text if item.find('source') is not None else 'Google News'

        news_list.append({
            'keyword': keyword,
            'title': title,
            'link': link,
            'pubDate': pub_date,
            'source': source
        })
    
    return news_list

def parse_news(news_list):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
    """
    parsed_news = []
    
    for news_item in news_list:
        title = news_item['title']
        pub_date_str = news_item['pubDate']
        keyword = news_item.get('keyword', 'N/A')
        source = news_item.get('source', 'N/A')
        link = news_item.get('link', 'N/A')

        try:
            dt = datetime.strptime(pub_date_str, '%a, %d %b %Y %H:%M:%S %Z')
            current_date = dt.strftime("%Y-%m-%d")
            time_str = dt.strftime("%H:%M:%S")
        except ValueError:
            current_date = date.today().strftime("%Y-%m-%d")
            time_str = "N/A"

        parsed_news.append({
            'keyword': keyword,
            'date': current_date,
            'time': time_str,
            'title': title,
            'source': source,
            'link': link
        })

    return parsed_news

def analyze_sentiment_lexicon(title):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏î‡πâ‡∏ß‡∏¢ Lexicon-based approach (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß)
    """
    # 1. Tokenization
    tokens = word_tokenize(title, engine='newmm')
    
    # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
    total_score = 0
    word_count = 0
    matched_words = []
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö negation (‡πÑ‡∏°‡πà, ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà, ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ)
    negation = False
    negation_words = ['‡πÑ‡∏°‡πà', '‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà', '‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ', '‡∏°‡∏¥', '‡∏°‡∏¥‡πÉ‡∏ä‡πà']
    
    for i, token in enumerate(tokens):
        token = token.strip()
        
        # ‡∏Ç‡πâ‡∏≤‡∏° stopwords ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        if not token or len(token) < 2:
            continue
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ negation
        if token in negation_words:
            negation = True
            continue
        
        # ‡∏Ç‡πâ‡∏≤‡∏° stopwords ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Ç‡πâ‡∏≤‡∏° sentiment words)
        if token in THAI_STOPWORDS and token not in THAI_SENTIMENT_LEXICON:
            continue
        
        # ‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å lexicon
        score = THAI_SENTIMENT_LEXICON.get(token, 0)
        
        if score != 0:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢
            if negation:
                score = -score
                negation = False
            
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏≥‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (intensifiers) ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á
            if i + 1 < len(tokens):
                next_token = tokens[i + 1].strip()
                intensifier = THAI_SENTIMENT_LEXICON.get(next_token, 1.0)
                if intensifier > 1.0:  # ‡πÄ‡∏õ‡πá‡∏ô intensifier
                    score = score * (intensifier / 1.0)
            
            total_score += score
            word_count += 1
            matched_words.append(f"{token}({score:.2f})")
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì polarity ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
    polarity = total_score / word_count if word_count > 0 else 0
    
    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
    polarity = max(-1.0, min(1.0, polarity))
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î label
    if polarity > 0.1:
        label = 'positive'
    elif polarity < -0.1:
        label = 'negative'
    else:
        label = 'neutral'
    
    return polarity, label, matched_words

def analyze_sentiment(parsed_news):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏Ç‡∏≠‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    analyzed_news = []
    
    for news in parsed_news:
        title = news['title']
        polarity, label, matched_words = analyze_sentiment_lexicon(title)
        
        news['sentiment'] = polarity
        news['sentiment_label'] = label
        news['matched_words'] = ', '.join(matched_words) if matched_words else '‡πÑ‡∏°‡πà‡∏°‡∏µ'
        
        analyzed_news.append(news)
    
    return analyzed_news

def save_results(df, keyword, output_dir='results'):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_keyword = keyword.replace(" ", "_").replace("/", "_")
    base_filename = f"{output_dir}/{safe_keyword}_{timestamp}"
    
    saved_files = {}
    
    # 1. CSV (UTF-8 with BOM for Excel compatibility)
    try:
        csv_file = f"{base_filename}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        saved_files['csv'] = csv_file
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å CSV: {csv_file}")
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å CSV: {e}")
    
    # 2. Excel
    try:
        excel_file = f"{base_filename}.xlsx"
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Analysis')
        saved_files['excel'] = excel_file
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Excel: {excel_file}")
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Excel: {e}")
    
    # 3. JSON
    try:
        json_file = f"{base_filename}.json"
        df.to_json(json_file, orient='records', force_ascii=False, indent=2)
        saved_files['json'] = json_file
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON: {json_file}")
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON: {e}")
    
    # 4. Summary Report
    try:
        summary_file = f"{base_filename}_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"{'='*70}\n")
            f.write(f"üìä ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Sentiment: {keyword}\n")
            f.write(f"{'='*70}\n\n")
            
            f.write(f"üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"üì∞ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(df)} ‡∏Ç‡πà‡∏≤‡∏ß\n\n")
            
            f.write(f"{'='*70}\n")
            f.write("üìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ Sentiment\n")
            f.write(f"{'='*70}\n")
            f.write(f"‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢:     {df['sentiment'].mean():.4f}\n")
            f.write(f"‡∏Ñ‡πà‡∏≤‡∏°‡∏±‡∏ò‡∏¢‡∏ê‡∏≤‡∏ô:   {df['sentiment'].median():.4f}\n")
            f.write(f"‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î:    {df['sentiment'].max():.4f}\n")
            f.write(f"‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î:    {df['sentiment'].min():.4f}\n")
            f.write(f"‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô: {df['sentiment'].std():.4f}\n\n")
            
            f.write(f"{'='*70}\n")
            f.write("üìä ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ Sentiment\n")
            f.write(f"{'='*70}\n")
            sentiment_counts = df['sentiment_label'].value_counts()
            for label, count in sentiment_counts.items():
                pct = (count / len(df)) * 100
                emoji = 'üü¢' if label == 'positive' else 'üî¥' if label == 'negative' else '‚ö™'
                f.write(f"{emoji} {label.upper():12s}: {count:3d} ‡∏Ç‡πà‡∏≤‡∏ß ({pct:5.1f}%)\n")
            
            f.write(f"\n{'='*70}\n")
            f.write("üü¢ TOP 5 ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ Sentiment ‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n")
            f.write(f"{'='*70}\n")
            top_positive = df.nlargest(5, 'sentiment')
            for idx, (i, row) in enumerate(top_positive.iterrows(), 1):
                f.write(f"\n{idx}. [{row['sentiment']:.3f}] {row['title'][:150]}\n")
                f.write(f"   üìÖ {row['date']} | üè¢ {row['source']}\n")
                f.write(f"   üî§ ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö: {row['matched_words']}\n")
            
            f.write(f"\n{'='*70}\n")
            f.write("üî¥ TOP 5 ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ Sentiment ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n")
            f.write(f"{'='*70}\n")
            top_negative = df.nsmallest(5, 'sentiment')
            for idx, (i, row) in enumerate(top_negative.iterrows(), 1):
                f.write(f"\n{idx}. [{row['sentiment']:.3f}] {row['title'][:150]}\n")
                f.write(f"   üìÖ {row['date']} | üè¢ {row['source']}\n")
                f.write(f"   üî§ ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö: {row['matched_words']}\n")
            
            f.write(f"\n{'='*70}\n")
            f.write("üì∞ ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n")
            f.write(f"{'='*70}\n")
            source_counts = df['source'].value_counts().head(10)
            for source, count in source_counts.items():
                f.write(f"  ‚Ä¢ {source}: {count} ‡∏Ç‡πà‡∏≤‡∏ß\n")
        
        saved_files['summary'] = summary_file
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Summary: {summary_file}")
    except Exception as e:
        print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Summary: {e}")
    
    return saved_files

def plot_sentiment(df, ticker, avg_sentiment, save_fig=True, output_dir='results'):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_ticker = ticker.replace(" ", "_").replace("/", "_")
    
    # ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà 1: Scatter plot
    fig, ax = plt.subplots(figsize=(14, 6))
    df['news_num'] = range(1, len(df) + 1)
    
    colors = df['sentiment'].apply(lambda x: '#4CAF50' if x > 0.1 else '#F44336' if x < -0.1 else '#FFC107')
    ax.scatter(df['news_num'], df['sentiment'], alpha=0.6, s=80, c=colors, edgecolors='black', linewidth=0.5)
    ax.plot(df['news_num'], df['sentiment'], alpha=0.3, linestyle='-', color='gray')
    ax.axhline(y=avg_sentiment, color='blue', linestyle='--', linewidth=2, 
               label=f'‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_sentiment:.3f}')
    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏≤‡∏á (0)')
    
    ax.set_title(f'‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Sentiment: {ticker}', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß (‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤)', fontsize=12)
    ax.set_ylabel('‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Sentiment', fontsize=12)
    ax.legend(fontsize=11, loc='best')
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # Annotate outliers
    for idx, row in df.iterrows():
        if abs(row['sentiment']) > 0.7:
            ax.annotate(f"{row['news_num']}", 
                       (row['news_num'], row['sentiment']),
                       textcoords="offset points", 
                       xytext=(0, 10 if row['sentiment'] > 0 else -15),
                       ha='center', fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    if save_fig:
        fig_file = f"{output_dir}/{safe_ticker}_{timestamp}_scatter.png"
        plt.savefig(fig_file, dpi=300, bbox_inches='tight')
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü: {fig_file}")
    plt.show()
    
    # ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà 2: Histogram
    fig, ax = plt.subplots(figsize=(12, 6))
    n, bins, patches = ax.hist(df['sentiment'], bins=25, edgecolor='black', alpha=0.7)
    
    # ‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏™‡∏µ‡∏ï‡∏≤‡∏° sentiment
    for i, patch in enumerate(patches):
        if bins[i] > 0.1:
            patch.set_facecolor('#4CAF50')
        elif bins[i] < -0.1:
            patch.set_facecolor('#F44336')
        else:
            patch.set_facecolor('#FFC107')
    
    ax.axvline(x=avg_sentiment, color='blue', linestyle='--', linewidth=2, 
               label=f'‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_sentiment:.3f}')
    ax.axvline(x=0, color='red', linestyle='--', linewidth=1, alpha=0.5)
    
    ax.set_title(f'‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Sentiment: {ticker}', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Sentiment', fontsize=12)
    ax.set_ylabel('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß', fontsize=12)
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    if save_fig:
        fig_file = f"{output_dir}/{safe_ticker}_{timestamp}_histogram.png"
        plt.savefig(fig_file, dpi=300, bbox_inches='tight')
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü: {fig_file}")
    plt.show()
    
    # ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà 3: Pie Chart
    fig, ax = plt.subplots(figsize=(10, 8))
    positive = (df['sentiment'] > 0.1).sum()
    negative = (df['sentiment'] < -0.1).sum()
    neutral = ((df['sentiment'] >= -0.1) & (df['sentiment'] <= 0.1)).sum()
    
    sizes = [positive, negative, neutral]
    labels = ['Positive', 'Negative', 'Neutral']
    colors_pie = ['#4CAF50', '#F44336', '#FFC107']
    explode = (0.05, 0.05, 0)
    
    wedges, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=colors_pie,
                                        autopct='%1.1f%%', startangle=90, textprops={'fontsize': 12})
    
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(14)
    
    ax.set_title(f'‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô Sentiment: {ticker}\n(‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡∏Ç‡πà‡∏≤‡∏ß)', 
                 fontsize=16, fontweight='bold', pad=20)
    
    plt.tight_layout()
    if save_fig:
        fig_file = f"{output_dir}/{safe_ticker}_{timestamp}_pie.png"
        plt.savefig(fig_file, dpi=300, bbox_inches='tight')
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü: {fig_file}")
    plt.show()

def compare_multiple_keywords(results_dict, output_dir='results'):
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô
    """
    comparison_data = []
    
    for keyword, df in results_dict.items():
        avg_sent = df['sentiment'].mean()
        pos_pct = ((df['sentiment'] > 0.1).sum() / len(df)) * 100
        neg_pct = ((df['sentiment'] < -0.1).sum() / len(df)) * 100
        neu_pct = 100 - pos_pct - neg_pct
        
        comparison_data.append({
            'keyword': keyword,
            'avg_sentiment': avg_sent,
            'positive_pct': pos_pct,
            'negative_pct': neg_pct,
            'neutral_pct': neu_pct,
            'total_news': len(df),
            'max_sentiment': df['sentiment'].max(),
            'min_sentiment': df['sentiment'].min()
        })
    
    comp_df = pd.DataFrame(comparison_data)
    
    # ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: Bar Chart
    fig, ax = plt.subplots(figsize=(14, 7))
    x = range(len(comp_df))
    colors_bar = ['#4CAF50' if val > 0 else '#F44336' if val < 0 else '#FFC107' 
                  for val in comp_df['avg_sentiment']]
    
    bars = ax.bar(x, comp_df['avg_sentiment'], color=colors_bar, alpha=0.7, edgecolor='black', linewidth=1.5)
    ax.axhline(y=0, color='black', linestyle='-', linewidth=1)
    
    ax.set_title('‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ Sentiment ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô', 
                 fontsize=18, fontweight='bold', pad=20)
    ax.set_xlabel('‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô', fontsize=13)
    ax.set_ylabel('‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ Sentiment', fontsize=13)
    ax.set_xticks(x)
    ax.set_xticklabels(comp_df['keyword'], rotation=30, ha='right', fontsize=11)
    ax.grid(True, alpha=0.3, axis='y', linestyle='--')
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏ö‡∏ô bar
    for bar, val in zip(bars, comp_df['avg_sentiment']):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{val:.3f}',
               ha='center', va='bottom' if height > 0 else 'top', 
               fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    fig_file = f"{output_dir}/comparison_{timestamp}_bar.png"
    plt.savefig(fig_file, dpi=300, bbox_inches='tight')
    print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: {fig_file}")
    plt.show()
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    comp_file = f"{output_dir}/comparison_{timestamp}.csv"
    comp_df.to