import requests
from bs4 import BeautifulSoup
from textblob import TextBlob
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, date
import xml.etree.ElementTree as ET
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_stopwords
import json
import os

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ matplotlib ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
plt.rcParams['font.family'] = 'TH Sarabun New'

# ‡∏Ç‡∏¢‡∏≤‡∏¢ Thai Sentiment Lexicon
THAI_SENTIMENT_LEXICON = {
    # ‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å‡∏°‡∏≤‡∏Å (0.8-1.0)
    "‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°": 1.0, "‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏¢‡∏≠‡∏î": 1.0, "‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î": 1.0, "‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°": 1.0, 
    "‡πÄ‡∏à‡∏£‡∏¥‡∏ç": 0.9, "‡∏£‡∏∏‡πà‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡∏á": 0.9, "‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï": 0.9, "‡∏û‡∏∏‡πà‡∏á": 0.9, "‡∏ó‡∏∞‡∏¢‡∏≤‡∏ô": 0.9,
    "‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à": 0.8, "‡∏ä‡∏ô‡∏∞": 0.8, "‡∏î‡∏µ": 0.8, "‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°": 0.9, "‡∏¢‡∏≠‡∏î": 0.8,
    # ‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (0.4-0.7)
    "‡∏ä‡∏≠‡∏ö": 0.7, "‡∏û‡∏≠‡πÉ‡∏à": 0.7, "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ": 0.7, "‡∏î‡∏µ‡πÉ‡∏à": 0.7, "‡∏™‡∏î‡πÉ‡∏™": 0.7,
    "‡∏Ç‡∏∂‡πâ‡∏ô": 0.6, "‡πÄ‡∏û‡∏¥‡πà‡∏°": 0.6, "‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô": 0.6, "‡∏ü‡∏∑‡πâ‡∏ô‡∏ï‡∏±‡∏ß": 0.6, "‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á": 0.6,
    "‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á": 0.5, "‡∏£‡∏≤‡∏ö‡∏£‡∏∑‡πà‡∏ô": 0.5,
    # ‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö‡∏°‡∏≤‡∏Å (-0.8 ‡∏ñ‡∏∂‡∏á -1.0)
    "‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å": -1.0, "‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß": -1.0, "‡πÄ‡∏à‡πä‡∏á": -1.0, "‡∏•‡πà‡∏°‡∏™‡∏•‡∏≤‡∏¢": -1.0, "‡∏ß‡∏¥‡∏Å‡∏§‡∏ï": -1.0,
    "‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï": -0.9, "‡πÇ‡∏Å‡∏á": -0.9, "‡∏â‡πâ‡∏≠‡πÇ‡∏Å‡∏á": -0.9, "‡∏´‡∏•‡∏≠‡∏Å‡∏•‡∏ß‡∏á": -0.9,
    "‡∏Ç‡∏≤‡∏î‡∏ó‡∏∏‡∏ô": -0.9, "‡∏ï‡∏Å‡∏ï‡πà‡∏≥": -0.9, "‡∏¢‡πà‡∏≥‡πÅ‡∏¢‡πà": -0.9, "‡∏î‡∏¥‡πà‡∏á": -0.9,
    "‡πÅ‡∏¢‡πà": -0.8, "‡∏™‡πÅ‡∏Å‡∏°": -0.8, "‡∏™‡πÅ‡∏Å‡∏°‡πÄ‡∏°‡∏≠‡∏£‡πå": -0.8, "‡πÄ‡∏™‡∏µ‡∏¢": -0.8, "‡πÄ‡∏•‡∏ß‡∏£‡πâ‡∏≤‡∏¢": -0.8,
    # ‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (-0.4 ‡∏ñ‡∏∂‡∏á -0.7)
    "‡∏õ‡∏±‡∏ç‡∏´‡∏≤": -0.7, "‡∏Å‡∏±‡∏á‡∏ß‡∏•": -0.7, "‡∏´‡πà‡∏ß‡∏á": -0.7, "‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á": -0.7, "‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢": -0.7,
    "‡∏•‡∏î‡∏•‡∏á": -0.6, "‡∏•‡∏î": -0.6, "‡∏´‡∏î": -0.6, "‡∏ï‡∏Å": -0.6, "‡∏•‡∏á": -0.6,
    "‡∏≠‡πà‡∏≠‡∏ô‡πÅ‡∏≠": -0.5, "‡∏ä‡∏∞‡∏•‡∏≠": -0.5, "‡∏ã‡∏ö‡πÄ‡∏ã‡∏≤": -0.5, "‡∏ï‡∏¥‡∏î‡∏Ç‡∏±‡∏î": -0.5,
    # ‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô
    "‡∏Å‡∏≥‡πÑ‡∏£": 0.8, "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ": 0.6, "‡∏•‡∏á‡∏ó‡∏∏‡∏ô": 0.5,
    "‡∏´‡∏ô‡∏µ‡πâ": -0.6, "‡∏Ç‡∏≤‡∏î‡∏î‡∏∏‡∏•": -0.7, "‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏ü‡πâ‡∏≠": -0.6, "‡∏ß‡πà‡∏≤‡∏á‡∏á‡∏≤‡∏ô": -0.7,
    # ‡∏´‡∏∏‡πâ‡∏ô
    "‡∏Ç‡∏≤‡∏¢": -0.3, "‡∏ñ‡∏∑‡∏≠": 0.1, "‡∏ã‡∏∑‡πâ‡∏≠": 0.4, "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ã‡∏∑‡πâ‡∏≠": 0.7,
    # ‡∏Ñ‡∏≥‡πÄ‡∏™‡∏£‡∏¥‡∏°
    "‡∏°‡∏≤‡∏Å": 1.2, "‡∏™‡∏∏‡∏î": 1.3, "‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î": 1.3, "‡πÑ‡∏°‡πà": -1.5,
}

THAI_STOPWORDS = set(thai_stopwords())

def get_google_news(keyword, lang="th", max_results=100):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å Google News"""
    if lang == "th":
        url = f"https://news.google.com/rss/search?q={keyword}&hl=th&gl=TH&ceid=TH:th"
    elif lang == "en":
        url = f"https://news.google.com/rss/search?q={keyword}&hl=en-US&gl=US&ceid=US:en"
    else:
        print("‚ö†Ô∏è ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: 'th' ‡∏´‡∏£‡∏∑‡∏≠ 'en'")
        return []

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error: {e}")
        return []
    
    try:
        root = ET.fromstring(response.content)
    except ET.ParseError as e:
        print(f"‚ùå XML Error: {e}")
        return []
    
    news_list = []
    for item in root.findall('./channel/item'):
        if len(news_list) >= max_results:
            break
        title = item.find('title').text if item.find('title') is not None else 'N/A'
        link = item.find('link').text if item.find('link') is not None else 'N/A'
        pub_date = item.find('pubDate').text if item.find('pubDate') is not None else 'N/A'
        source = item.find('source').text if item.find('source') is not None else 'Google News'
        news_list.append({'keyword': keyword, 'title': title, 'link': link, 
                         'pubDate': pub_date, 'source': source})
    return news_list

def parse_news(news_list):
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß"""
    parsed_news = []
    for news_item in news_list:
        try:
            dt = datetime.strptime(news_item['pubDate'], '%a, %d %b %Y %H:%M:%S %Z')
            current_date = dt.strftime("%Y-%m-%d")
            time_str = dt.strftime("%H:%M:%S")
        except:
            current_date = date.today().strftime("%Y-%m-%d")
            time_str = "N/A"
        parsed_news.append({
            'keyword': news_item.get('keyword', 'N/A'),
            'date': current_date, 'time': time_str,
            'title': news_item['title'],
            'source': news_item.get('source', 'N/A'),
            'link': news_item.get('link', 'N/A')
        })
    return parsed_news

def analyze_sentiment_lexicon(title):
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏î‡πâ‡∏ß‡∏¢ Lexicon"""
    tokens = word_tokenize(title, engine='newmm')
    total_score, word_count = 0, 0
    matched_words, negation = [], False
    
    for i, token in enumerate(tokens):
        token = token.strip()
        if not token or len(token) < 2:
            continue
        if token in ['‡πÑ‡∏°‡πà', '‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà', '‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ', '‡∏°‡∏¥']:
            negation = True
            continue
        if token in THAI_STOPWORDS and token not in THAI_SENTIMENT_LEXICON:
            continue
        
        score = THAI_SENTIMENT_LEXICON.get(token, 0)
        if score != 0:
            if negation:
                score = -score
                negation = False
            if i + 1 < len(tokens):
                next_token = tokens[i + 1].strip()
                intensifier = THAI_SENTIMENT_LEXICON.get(next_token, 1.0)
                if intensifier > 1.0:
                    score *= (intensifier / 1.0)
            total_score += score
            word_count += 1
            matched_words.append(f"{token}({score:.2f})")
    
    polarity = max(-1.0, min(1.0, total_score / word_count if word_count > 0 else 0))
    label = 'positive' if polarity > 0.1 else 'negative' if polarity < -0.1 else 'neutral'
    return polarity, label, matched_words

def analyze_sentiment(parsed_news):
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    analyzed_news = []
    for news in parsed_news:
        polarity, label, matched = analyze_sentiment_lexicon(news['title'])
        news['sentiment'] = polarity
        news['sentiment_label'] = label
        news['matched_words'] = ', '.join(matched) if matched else '‡πÑ‡∏°‡πà‡∏°‡∏µ'
        analyzed_news.append(news)
    return analyzed_news

def save_results(df, keyword, output_dir='results'):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"""
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_keyword = keyword.replace(" ", "_").replace("/", "_")
    base = f"{output_dir}/{safe_keyword}_{timestamp}"
    
    # CSV
    try:
        df.to_csv(f"{base}.csv", index=False, encoding='utf-8-sig')
        print(f"‚úÖ CSV: {base}.csv")
    except Exception as e:
        print(f"‚ö†Ô∏è CSV Error: {e}")
    
    # Excel
    try:
        df.to_excel(f"{base}.xlsx", index=False, engine='openpyxl')
        print(f"‚úÖ Excel: {base}.xlsx")
    except Exception as e:
        print(f"‚ö†Ô∏è Excel Error: {e}")
    
    # JSON
    try:
        df.to_json(f"{base}.json", orient='records', force_ascii=False, indent=2)
        print(f"‚úÖ JSON: {base}.json")
    except Exception as e:
        print(f"‚ö†Ô∏è JSON Error: {e}")
    
    # Summary
    try:
        with open(f"{base}_summary.txt", 'w', encoding='utf-8') as f:
            f.write(f"{'='*70}\nüìä ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Sentiment: {keyword}\n{'='*70}\n\n")
            f.write(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß: {len(df)}\n")
            f.write(f"‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {df['sentiment'].mean():.4f}\n")
            f.write(f"‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {df['sentiment'].max():.4f}\n")
            f.write(f"‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: {df['sentiment'].min():.4f}\n\n")
            counts = df['sentiment_label'].value_counts()
            for label, count in counts.items():
                f.write(f"{label}: {count} ({count/len(df)*100:.1f}%)\n")
            f.write(f"\n{'='*70}\nTOP 3 ‡∏ö‡∏ß‡∏Å:\n")
            for idx, row in df.nlargest(3, 'sentiment').iterrows():
                f.write(f"{row['sentiment']:.3f} - {row['title'][:100]}\n")
            f.write(f"\n{'='*70}\nTOP 3 ‡∏•‡∏ö:\n")
            for idx, row in df.nsmallest(3, 'sentiment').iterrows():
                f.write(f"{row['sentiment']:.3f} - {row['title'][:100]}\n")
        print(f"‚úÖ Summary: {base}_summary.txt")
    except Exception as e:
        print(f"‚ö†Ô∏è Summary Error: {e}")

def plot_sentiment(df, ticker, avg_sentiment, save_fig=True, output_dir='results'):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_ticker = ticker.replace(" ", "_").replace("/", "_")
    df['news_num'] = range(1, len(df) + 1)
    
    # Scatter
    fig, ax = plt.subplots(figsize=(14, 6))
    colors = df['sentiment'].apply(lambda x: '#4CAF50' if x > 0.1 else '#F44336' if x < -0.1 else '#FFC107')
    ax.scatter(df['news_num'], df['sentiment'], alpha=0.6, s=80, c=colors, edgecolors='black')
    ax.plot(df['news_num'], df['sentiment'], alpha=0.3, color='gray')
    ax.axhline(avg_sentiment, color='blue', linestyle='--', linewidth=2, label=f'‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_sentiment:.3f}')
    ax.axhline(0, color='red', linestyle='--', alpha=0.5, label='‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏≤‡∏á')
    ax.set_title(f'Sentiment Analysis: {ticker}', fontsize=16, fontweight='bold')
    ax.set_xlabel('‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß', fontsize=12)
    ax.set_ylabel('Sentiment', fontsize=12)
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    if save_fig:
        plt.savefig(f"{output_dir}/{safe_ticker}_{timestamp}_scatter.png", dpi=300)
    plt.show()
    
    # Histogram
    fig, ax = plt.subplots(figsize=(12, 6))
    n, bins, patches = ax.hist(df['sentiment'], bins=25, edgecolor='black', alpha=0.7)
    for i, patch in enumerate(patches):
        patch.set_facecolor('#4CAF50' if bins[i] > 0.1 else '#F44336' if bins[i] < -0.1 else '#FFC107')
    ax.axvline(avg_sentiment, color='blue', linestyle='--', linewidth=2, label=f'‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_sentiment:.3f}')
    ax.set_title(f'‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ Sentiment: {ticker}', fontsize=16, fontweight='bold')
    ax.set_xlabel('Sentiment', fontsize=12)
    ax.set_ylabel('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß', fontsize=12)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    if save_fig:
        plt.savefig(f"{output_dir}/{safe_ticker}_{timestamp}_histogram.png", dpi=300)
    plt.show()
    
    # Pie
    fig, ax = plt.subplots(figsize=(10, 8))
    pos = (df['sentiment'] > 0.1).sum()
    neg = (df['sentiment'] < -0.1).sum()
    neu = len(df) - pos - neg
    wedges, texts, autotexts = ax.pie([pos, neg, neu], labels=['Positive', 'Negative', 'Neutral'],
                                       colors=['#4CAF50', '#F44336', '#FFC107'],
                                       autopct='%1.1f%%', startangle=90)
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
    ax.set_title(f'‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô Sentiment: {ticker}', fontsize=16, fontweight='bold')
    plt.tight_layout()
    if save_fig:
        plt.savefig(f"{output_dir}/{safe_ticker}_{timestamp}_pie.png", dpi=300)
    plt.show()

def compare_keywords(results_dict, output_dir='results'):
    """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô"""
    data = []
    for kw, df in results_dict.items():
        data.append({
            'keyword': kw,
            'avg': df['sentiment'].mean(),
            'pos_pct': (df['sentiment'] > 0.1).sum() / len(df) * 100,
            'neg_pct': (df['sentiment'] < -0.1).sum() / len(df) * 100,
            'total': len(df)
        })
    comp_df = pd.DataFrame(data)
    
    fig, ax = plt.subplots(figsize=(14, 7))
    colors = ['#4CAF50' if v > 0 else '#F44336' if v < 0 else '#FFC107' for v in comp_df['avg']]
    bars = ax.bar(range(len(comp_df)), comp_df['avg'], color=colors, alpha=0.7, edgecolor='black')
    ax.axhline(0, color='black', linestyle='-')
    ax.set_title('‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Sentiment', fontsize=18, fontweight='bold')
    ax.set_xticks(range(len(comp_df)))
    ax.set_xticklabels(comp_df['keyword'], rotation=30, ha='right')
    ax.set_ylabel('‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ Sentiment')
    ax.grid(True, alpha=0.3, axis='y')
    for bar, val in zip(bars, comp_df['avg']):
        ax.text(bar.get_x() + bar.get_width()/2, val, f'{val:.3f}',
               ha='center', va='bottom' if val > 0 else 'top', fontweight='bold')
    plt.tight_layout()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plt.savefig(f"{output_dir}/comparison_{timestamp}.png", dpi=300)
    print(f"‚úÖ ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: {output_dir}/comparison_{timestamp}.png")
    plt.show()
    
    comp_df.to_csv(f"{output_dir}/comparison_{timestamp}.csv", index=False, encoding='utf-8-sig')
    print(f"‚úÖ ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö: {output_dir}/comparison_{timestamp}.csv")
    return comp_df

def main(ticker, lang="th", max_results=100, save_files=True):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    print(f"\n{'='*60}\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß: {ticker}\n{'='*60}")
    news_list = get_google_news(ticker, lang=lang, max_results=max_results)
    if not news_list:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{ticker}'")
        return None
    
    print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏î‡πâ: {len(news_list)} ‡∏Ç‡πà‡∏≤‡∏ß")
    parsed = parse_news(news_list)
    analyzed = analyze_sentiment(parsed)
    df = pd.DataFrame(analyzed)
    avg_sentiment = df['sentiment'].mean()
    
    print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ:")
    print(f"  ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_sentiment:.4f}")
    counts = df['sentiment_label'].value_counts()
    for label, count in counts.items():
        print(f"  {label}: {count} ({count/len(df)*100:.1f}%)")
    
    if save_files:
        print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå...")
        save_results(df, ticker)
        plot_sentiment(df, ticker, avg_sentiment, save_fig=True)
    else:
        plot_sentiment(df, ticker, avg_sentiment, save_fig=False)
    
    return df

def analyze_multiple(keywords, lang="th", max_results=50, save_files=True):
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô"""
    results = {}
    for kw in keywords:
        df = main(kw, lang=lang, max_results=max_results, save_files=save_files)
        if df is not None:
            results[kw] = df
    
    if len(results) > 1:
        print(f"\n{'='*60}\nüìä ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n{'='*60}")
        comp_df = compare_keywords(results)
        return results, comp_df
    return results, None

# ===== ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô =====
if __name__ == "__main__":
    print("="*60)
    print("üáπüá≠ Thai Sentiment Analyzer")
    print("="*60)
    
    # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    # ticker = input("\n‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô: ").strip() or "‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢"
    # main(ticker, lang="th", max_results=50, save_files=True)
    
    # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 2: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥ (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
    keywords = ["‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢ SET", "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥", "‡∏≠‡∏™‡∏±‡∏á‡∏´‡∏≤‡∏£‡∏¥‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå", "‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÑ‡∏ó‡∏¢"]
    
    # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏™‡πà‡πÄ‡∏≠‡∏á
    # user_input = input("\n‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ,): ").strip()
    # if user_input:
    #     keywords = [k.strip() for k in user_input.split(',')]
    
    results, comparison = analyze_multiple(keywords, lang="th", max_results=50, save_files=True)
    
    print("\n" + "="*60)
    print("‚ú® ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
    print("="*60)
    print("üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô folder 'results/'")
    print("  - CSV, Excel, JSON")
    print("  - Summary.txt")
    print("  - ‡∏Å‡∏£‡∏≤‡∏ü PNG")